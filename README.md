# 전체 플로우
## 1. 데이터 수집
## 2. 데이터 전처리 1 - 수집한 두 데이터를 하나로 합치기(all_raw_data.csv로 저장)
## 3. 데이터 전처리 2 - all_raw_data 토큰화, 렉사일 지수를 적절한 기준에 따라 범주화 (final_data.pkl로 저장)
## 4. 데이터 전처리 3 - final_data.pkl을 BERT에 입력, 결과물 저장 (bert_embeddings.pkl로 저장)
## 5. 데이터 전처리 4 - train_test_split, 데이터 텐서로 변환
## 6. 모델 구축
## 7. 모델 훈련 및 최선의 모델 저장
## 8. 모델 평가


모델 구축 방법

BERT head + FC layer(Fully connected layer)

BERT의 파라미터는 그대로 고정하고, FC layer만 학습시키는 방법
시간 단축, 컴퓨팅 리소스 절약할 수 있는 동시에 BERT라는 강력한 자연어 처리 모델을 활용할 수 있다는 점에서 해당 방법론 선택

우리가 지금 진행하고자 하는 방식은 BERT의 파라미터를 고정시키고, FC layer만 학습하는 방식이므로
BERT에 데이터를 통과시킨 후 결과 데이터를 가지고 FC layer를 학습시키는 데 집중할 것임

원래 하던 방식은, 토큰화된 데이터를 BERT와 FC layer 모두 통과시키는 방법으로 학습했음
각 에포크마다 데이터를 BERT에 다시 통과시켜야 하기 때문에 효율이 떨어짐

1. 토큰화된 데이터를 BERT에 입력한다
2. BERT에서 출력된 결과물을 가지고 FC layer를 학습시킨다.
3. FC layer에만 집중할 수 있으므로, 
    에포크 수를 늘리던가(BERT가 포함되면 에포크 수는 3~5정도까지밖에 안 됨) 여러가지 방법을 가지고
    정확도와 효율을 동시에 잡을 것

## 1. 데이터 수집

카글(kaggle)과 github(eannefawcett)에서 공개된 데이터 이용


카글 자료 출처 = https://www.kaggle.com/datasets/kononenko/commonlit-texts
commonlit_texts.csv을 저장

github(제작자명: eannefawcett) = https://github.com/eannefawcett/lexile-determination-v2/tree/master/data/raw
labeled_data.pickle을 저장

1. 구글 드라이브의 내 드라이브 - (만약 없다면 생성) Colab Notebooks 폴더 - LEXILE 폴더 생성 - data 폴더 생성
2. 이 두 데이터를 1.의 해당 경로 안에 업로드
3. 코랩을 구글 드라이브와 연결하여 데이터 불러오기


##2. 데이터 전처리 1 - 수집한 두 데이터셋을 하나의 데이터셋(all_raw_data.csv)으로 만들기

단,

1. 렉사일 지수, 그리고 글을 column으로 한 데이터 프레임으로 만들어야 함
2. 렉사일 지수는 정수형 데이터여야 함
3. 결측치가 없어야 함

###2-1 commonlit에서 lexile 열과 text열만 추출
###2-2 labeled_data에서 lexile 열과 text열만 추출
+ (추가적으로) lexile열의 데이터들이 알파벳이 포함된 문자열임 -> 수정하기

###2-3 all_raw_data로 합치기 + csv 형식으로 저장

## 3. 데이터 전처리 2 - all_raw_data 토큰화, 렉사일 지수를 적절한 기준에 따라 

*   항목 추가
*   항목 추가

범주화 (final_data.pkl로 저장)



###1. 토큰화 방법
- BERT는 한 번에 512토큰까지 처리 가능함
- BERT는 CLS 토큰과 SEP 토큰으로 둘러싸여 있는 부분을 한 문장으로 인식
- 그러한 방식으로 최대 두 문장까지밖에 인식 불가
- 하지만 지금 all_raw_data.csv의 한 셀 안에 수많은 문장들이 들어 있음
- 즉 각각의 셀마다 512토큰을 훌쩍 넘는 길이의 글들이 3개 이상의 문장들로 존재
- 이를 적절히 해결하기 위해 다음과 같은 방법을 거칠 것
-- 하나의 셀이 512토큰을 넘으면 쪼개기
-- 실제로는 여러 문장들로 구성되어 있어도 앞뒤로 CLS 토큰과 SEP 토큰을 넣어서 한 문장인 것처럼 만들기

-- 하나의 셀이 512토큰을 넘으면 쪼개기 -> 이 경우 뒤에 남은 꼬다리 글의 길이가 너무 짧을 수 있음

-- 그게 아니더라도 길이가 너무 짧은 글들은 제거해야 함 (정보가 너무 없기 때문)

---> 해결 방법: 토큰 길이의 분포, 사분위수를 파악하여 토큰 길이의 하한선 설정, 기준 미달인 행 제거

----마지막으로 패딩, attention mask, token type id 추가


-----------------------------------------------------------------------------


###2. 렉사일 지수를 적절한 기준에 따라 범주화
- 회귀 모델을 구성하기보다 범주 예측 모델을 만드는 것이 더 적합함
- 우리의 궁극적인 목표는 적절한 난이도의 글을 학생에게 추천하는 것
- 범주의 수가 너무 적지만 않다면 범주 예측 모델로도 충분히 가능함
- 이에 비해 정확한 하나의 숫자로 예측하는 것에 대한 메리트가 크지 않음

-----그렇다면 어떤 기준에 따라 렉사일 지수 구간을 나눌 것인가?
1. 범주의 개수가 너무 적으면 안 됨 (예를 들어 2~4개 범주로 나누는 경우)
2. 각 범주마다 비슷한 개수의 데이터가 있어야 함 (성능이 좋은 모델을 만들기 위해)

이에 따라

- 렉사일 지수의 분포를 히스토그램으로 파악
- 시행착오 끝에 6개 범주, 각 경계값은 (100/6)% 퍼센타일로 하면 앞의 두 조건을 만족시킬 수 있다는 것을 알게 되었음 + 렉사일 지수 구간의 크기도 약 200씩으로 일정한 편
- 그렇게 범주화 진행, 렉사일 지수가 가장 작은 구간부터 0~5의 정수 부여 (다중 분류 모델을 학습시키기 위한 loss function으로 cross entropy가 주로 사용되며, 이 함수를 사용하기 위해서는 이런 방식으로 레이블을 붙여야 함


###3. all_raw_data 토큰화, 0~5로 범주화된 렉사일 지수를 (final_data.pkl로 저장)


## 4. 데이터 전처리 3 - final_data.pkl을 BERT에 입력, 결과물 저장 (bert_embeddings.pkl로 저장)
우리가 지금 진행하고자 하는 방식은 BERT의 파라미터를 고정시키고,
FC layer만 학습하는 방식

BERT에 데이터를 통과시킨 후 결과 데이터를 가지고
FC layer를 학습시키는 데 집중할 것임

처음에 생각했던 방식은,
토큰화된 데이터를 BERT와 FC layer 모두 통과시키는 방법으로 학습했음

각 에포크마다 데이터를 BERT에 다시 통과시켜야 하기 때문에 효율이 떨어짐

1. 토큰화된 데이터를 BERT에 입력한다
2. BERT에서 출력된 결과물을 가지고 FC layer를 학습시킨다.
3. FC layer에만 집중할 수 있으므로,
    에포크 수를 늘리던가(BERT가 포함되면 에포크 수는 3~5정도까지밖에 안 됨) 여러가지 방법을 가지고
    정확도와 효율을 동시에 잡을 것


## 5. 데이터 전처리 4 - train_test_split, 데이터 텐서로 변환
+6. 모델 구축
+7. 모델 훈련 및 최선의 모델 저장
+8. 모델 평가


7. 모델 훈련 및 최선의 모델 저장
8. 모델 평가

사용한 방법론들

1. 데이터 정규화 - BERT에서 출력된 결과물을 FC layer에 입력하기 전에 먼저 정규화를 시킴
                정규화는 (편차/표준편차)로 계산
                표준편차가 0이 되거나, 너무 심하게 compression되는 것을 막기 위해 분모에 미세한 수 (1e-8) 더함

2. train_test_split

3. FC layer에 입력하기 위해 데이터 탠서화

4. TensorDataset으로 데이터 묶기 + DataLoader(batch_size = 16)로 TensorDataset을 불러오기

5. 모델 설계
    입력 레이어 - 768차원

    첫 번째 FC Layer - 768차원 to 512차원
    활성화 함수 1 (Relu)
    드롭아웃 1 Dropout(p=0.1)

    두 번째 FC Layer - 512차원 to 256차원
    활성화 함수 2 (Relu)
    드롭아웃 2 Dropout(p=0.1)

    세 번째 FC Layer - 256차원 to 128차원
    활성화 함수 3 (Relu)
    드롭아웃 3 Dropout(p=0.1)

    네 번째 FC Layer - 128차원 to 6차원 (실제로 분류할 범주의 개수)
    LayerNorm 추가
        네 번째 FC Layer의 결과물인 Logits 값의 최댓값과 최솟값의 차이가 20을 넘어가면
        softmax 함수 통과할 때 비정상적인 값 출력
        따라서 비정상적으로 큰 loss 값을 가지게 됨
        최댓값과 최솟값의 차이를 줄이기 위해 마지막에 LayerNorm 추가하는 것

    맨 끝에 softmax를 추가해야 하는 것 아닌가요?
        crossEntropy loss는 이렇게만 놔둬도 내재된 softmax 활용하여 알아서 loss 값 계산
        나중에 실제로 사용할 때에는 softmax를 뒤에 덧붙여야 함

6. 모델 훈련
    learning rate = 0.0001
    epochs = 150
    early stopping 적용 (patience = 10)
    loss function = crossEntropy
    sceduler 활용하여 learning rate를 loss에 따라 자동적으로 적절히 적용
    Xavier initialization 적용하여 initialization도 최대한 잘 되게 적용
    중요 !!!! loss 계산이 평균으로 되어 있는지 확인해야 함 !!!! loss를 단순합으로 계산하고 있었음
        그러니 loss가 엄청 크게 나오지...
        

7. 모델 평가 및 저장
    accuracy = correct / total로 계산

    




